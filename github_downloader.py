#!/usr/bin/env python3
"""
GitHub Repository Downloader
Baixa todos os arquivos via Web/HTML Scraping (API-Free).
"""

import os
import sys
import requests
import time
import json
import re
import getpass
from urllib.parse import urlparse, unquote
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup

class GitHubDownloader:
    def __init__(self, repo_url: str, output_dir: str = None, branch: str = "main"):
        self.repo_url = repo_url.rstrip('/')
        self.branch = branch
        self.owner, self.repo = self._parse_repo_url()
        self.output_dir = output_dir or self.repo
        self.web_base = "https://github.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        })
        self.stats = {'files': 0, 'dirs': 0, 'errors': 0, 'size': 0, 'skipped': 0}
        self.visited_dirs = set()
        
        # Check connection and setup proxy if needed
        self._check_connection()

    def _check_connection(self):
        """Verifica conex√£o inicial e configura proxy autenticado se necess√°rio."""
        print("üîå Verificando conex√£o e proxy...")
        try:
            # Tenta conectar ao GitHub (usa proxies do sistema/env vars por padr√£o)
            self.session.get("https://github.com", timeout=10)
            print("   ‚úÖ Conex√£o inicial OK")
        except requests.exceptions.ProxyError as e:
            if "407" in str(e) or "authenticationrequired" in str(e).lower():
                self._handle_proxy_auth()
            else:
                self._handle_proxy_auth(force_prompt=True, error_msg=str(e))
        except requests.exceptions.SSLError:
            print("   ‚ö†Ô∏è  Erro de SSL detectado. Tentando ignorar verifica√ß√£o SSL (n√£o recomendado)...")
            self.session.verify = False
            self.session.get("https://github.com", timeout=10)
        except Exception as e:
            if "407" in str(e): # Pega 407 gen√©rico
                self._handle_proxy_auth()
            else:
                print(f"   ‚ö†Ô∏è  Aviso: Falha na conex√£o inicial: {e}")
                print("       O script tentar√° continuar, mas pode falhar.")

    def _handle_proxy_auth(self, force_prompt=False, error_msg=""):
        print(f"\nüö´ FALHA DE AUTENTICA√á√ÉO NO PROXY (407)")
        if error_msg:
             print(f"   Erro original: {error_msg}")
        print("   Suas vari√°veis de ambiente definem um proxy, mas ele exige usu√°rio e senha.")
        print("   O Python 'requests' n√£o pega credenciais do Windows automaticamente.\n")
        
        choice = input("üëâ Deseja inserir usu√°rio/senha do proxy agora? (S/n): ").strip().lower()
        if choice in ['n', 'nao', 'n√£o']:
            return

        proxy_host = input("üåê Proxy Host (ex: proxy.empresa.com:8080) [Enter para tentar detectar]: ").strip()
        user = input("üë§ Usu√°rio Proxy: ").strip()
        password = getpass.getpass("üîë Senha Proxy: ").strip()

        if not proxy_host:
            # Tenta pegar do environ se o usu√°rio n√£o digitar
            proxy_host = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY') or ""
            # Remove http:// e credenciais antigas se houver
            proxy_host = proxy_host.replace("http://", "").replace("https://", "").split("@")[-1]
        
        if not proxy_host:
             print("‚ùå Erro: Nenhum host de proxy encontrado ou fornecido.")
             return

        # Monta URL autenticada
        proxy_url = f"http://{user}:{password}@{proxy_host}"
        
        self.session.proxies = {
            "http": proxy_url,
            "https": proxy_url
        }
        
        print("\nüîÑ Retentando conex√£o com credenciais...")
        try:
            self.session.get("https://github.com", timeout=10)
            print("   ‚úÖ Autentica√ß√£o de Proxy: SUCESSO!")
        except Exception as e:
             print(f"   ‚ùå Falha na autentica√ß√£o do proxy: {e}")
             sys.exit(1)

    def _parse_repo_url(self) -> tuple:
        parsed = urlparse(self.repo_url)
        parts = parsed.path.strip('/').split('/')
        if len(parts) < 2:
            raise ValueError(f"URL inv√°lida: {self.repo_url}")
        return parts[0], parts[1].replace('.git', '')

    def _get_page_content(self, url: str) -> BeautifulSoup:
        """Baixa e parseia uma p√°gina HTML."""
        try:
            response = self.session.get(url, timeout=15)
            if response.status_code == 404:
                return None
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f"‚ùå Erro ao acessar {url}: {e}")
            return None

    def _extract_items_from_payload(self, soup: BeautifulSoup, current_path: str = "") -> list:
        """Tenta extrair itens de diret√≥rio do JSON interno (React) ou HTML fallback."""
        items = [] # list of (type, name, path, url)
        
        # 1. Tentar via JSON embutido (mais confi√°vel)
        for script in soup.find_all('script', type='application/json'):
            if 'tree' in script.text and 'items' in script.text:
                try:
                    data = json.loads(script.text)
                    payload = data.get('payload', {})
                    
                    # Navega√ß√£o na estrutura do payload
                    if 'tree' in payload and 'items' in payload['tree']:
                        for item in payload['tree']['items']:
                            name = item['name']
                            # Path completo
                            full_path = item['path'] 
                            # Determina tipo
                            content_type = item['contentType'] # 'directory' ou 'file'
                            
                            item_type = 'tree' if content_type == 'directory' else 'blob'
                            item_url = f"{self.web_base}/{self.owner}/{self.repo}/{item_type}/{self.branch}/{full_path}"
                            
                            items.append({
                                'type': item_type,
                                'path': full_path,
                                'url': item_url
                            })
                        return items
                except:
                    pass

        # 2. Fallback: Parsear HTML (tabela de arquivos)
        # Seletores comuns do GitHub
        links = soup.select('a.js-navigation-open')
        for link in links:
            href = link.get('href', '')
            if not href: continue
            
            # Filtros b√°sicos para ignorar links de navega√ß√£o
            if '/commit/' in href or '/commits/' in href or '/blame/' in href:
                continue
            if href.endswith('/..'):
                continue
                
            # Verifica se √© parte do repo
            repo_base = f"/{self.owner}/{self.repo}"
            if not href.startswith(repo_base):
                continue
                
            # Identifica blob (arquivo) ou tree (pasta)
            if f"/blob/{self.branch}/" in href:
                item_type = 'blob'
                rel_path = href.split(f"/blob/{self.branch}/", 1)[1]
            elif f"/tree/{self.branch}/" in href:
                item_type = 'tree'
                rel_path = href.split(f"/tree/{self.branch}/", 1)[1]
            else:
                continue
                
            items.append({
                'type': item_type,
                'path': unquote(rel_path),
                'url': self.web_base + href
            })
            
        return items

    def _scrape_file_content(self, url: str) -> str:
        """Extrai conte√∫do de arquivo do HTML (blob)."""
        try:
            soup = self._get_page_content(url)
            if not soup: return None
            
            # 1. Tentar via JSON Payload (React)
            for script in soup.find_all('script', type='application/json'):
                if 'rawLines' in script.text:
                    try:
                        data = json.loads(script.text)
                        if 'payload' in data and 'blob' in data['payload']:
                            blob = data['payload']['blob']
                            if 'rawLines' in blob:
                                return '\n'.join(blob['rawLines'])
                            if 'headerInfo' in blob and blob['headerInfo'].get('toc') is None:
                                # Pode ser bin√°rio se n√£o tiver rawLines
                                pass
                    except:
                        pass
            
            # 2. Tentar via Tabela HTML (legado/fallback)
            lines = []
            rows = soup.find_all('td', class_='blob-code-inner')
            if rows:
                for row in rows:
                    lines.append(row.get_text())
                return '\n'.join(lines)
            
            # 3. Tentar textarea (muito antigo)
            textarea = soup.find('textarea', {'id': 'read-only-cursor-text-area'})
            if textarea:
                return textarea.get_text()

            return None # Provavelmente bin√°rio
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Falha ao ler {url}: {e}")
            return None

    def crawl_directory(self, dir_url: str):
        """Navega recursivamente nos diret√≥rios."""
        if dir_url in self.visited_dirs:
            return
        self.visited_dirs.add(dir_url)
        
        soup = self._get_page_content(dir_url)
        if not soup: return

        # Extrai items
        items = self._extract_items_from_payload(soup)
        
        files_to_download = []
        dirs_to_visit = []

        for item in items:
            if item['type'] == 'blob':
                files_to_download.append(item)
            elif item['type'] == 'tree':
                dirs_to_visit.append(item)

        # Processa arquivos deste diret√≥rio
        if files_to_download:
            print(f"   üìÇ Processando pasta: {dir_url.split('/')[-1]} ({len(files_to_download)} arquivos)")
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {executor.submit(self._process_file, f): f for f in files_to_download}
                for f in as_completed(futures):
                    pass # Wait completion

        # Recurse (serial para n√£o sobrecarregar)
        for d in dirs_to_visit:
            self.crawl_directory(d['url'])

    def _process_file(self, item: dict):
        path = item['path']
        file_path = Path(self.output_dir) / path
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        if file_path.exists():
            return # Skip se j√° existe (opcional)

        content = self._scrape_file_content(item['url'])
        
        if content is not None:
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                self.stats['files'] += 1
                self.stats['size'] += len(content)
                print(f"      ‚úÖ {item['path']}")
            except Exception as e:
                print(f"      ‚ùå Erro ao salvar {item['path']}: {e}")
        else:
            self.stats['skipped'] += 1
            # print(f"      ‚è© Bin√°rio/Ignorado: {item['path']}")

    def start(self):
        print(f"\nüï∑Ô∏è  Iniciando Crawler (API-Free) para: {self.owner}/{self.repo}")
        print(f"Target: {self.web_base}/{self.owner}/{self.repo}/tree/{self.branch}")
        
        start_root = f"{self.web_base}/{self.owner}/{self.repo}/tree/{self.branch}"
        
        # Verifica se o link 'main' funciona, sen√£o tenta master
        if self._get_page_content(start_root) is None:
             if self.branch == 'main':
                 self.branch = 'master'
                 start_root = f"{self.web_base}/{self.owner}/{self.repo}/tree/{self.branch}"
                 print(f"‚ö†Ô∏è  Trocando para branch 'master'")
        
        start_time = time.time()
        self.crawl_directory(start_root)
        
        elapsed = time.time() - start_time
        print(f"\n{'='*50}")
        print(f"‚úÖ Conclu√≠do em {elapsed:.2f}s")
        print(f"   Arquivos: {self.stats['files']}")
        print(f"   Ignorados (Bin): {self.stats['skipped']}")
        print(f"   Local: {os.path.abspath(self.output_dir)}")
        print(f"{'='*50}\n")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso: python github_downloader.py <URL> [BRANCH] [DESTINO]")
        sys.exit(1)
        
    url = sys.argv[1]
    branch = sys.argv[2] if len(sys.argv) > 2 else "main"
    dest = sys.argv[3] if len(sys.argv) > 3 else None
    
    GitHubDownloader(url, dest, branch).start()
